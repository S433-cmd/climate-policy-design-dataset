---
title: "IEA OpenAI Example Codierung"
author: "Simon Rittershaus"
date: "2024-07-30"
output: html_document
---
# Setup
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(reticulate)
library(here)

# --- Python: keine Hardpaths im Repo ---
# Option 1: Nutze ein venv/conda, das lokal aktiviert ist
# Option 2 (portable): python via ENV setzen, z.B. RETICULATE_PYTHON
py_bin <- Sys.getenv("RETICULATE_PYTHON")
if (py_bin != "") {
  use_python(py_bin, required = TRUE)
} else {
  # fallback: nimm einfach den Python, den reticulate findet
  # (oder du setzt in README: export RETICULATE_PYTHON=...)
  message("RETICULATE_PYTHON not set; using default Python discovered by reticulate.")
}
py_config()

# --- OpenAI: nur LESEN, nie schreiben ---
apiKey <- Sys.getenv("OPENAI_API_KEY")
if (apiKey == "") stop("OPENAI_API_KEY is not set. See README: API key setup.")

apiOrg <- Sys.getenv("OPENAI_ORG")  # optional
apiProject <- Sys.getenv("OPENAI_PROJECT") # optional
```

# Set working directory and Check files
```{python}
import os
from pathlib import Path

# --- Repo-Root robust finden ---
# Annahme: dieses Notebook liegt irgendwo im Repo, wir gehen nach oben,
# bis wir ein typisches Marker-File/Folder finden.
# Minimal: Root = current working dir (beim Knit meist Projekt-Root) ODER Skript-Ordner.
cwd = Path.cwd()

# Wenn du sicher bist, dass du im Repo-Root knittest, reicht:
repo_root = cwd

# Wenn du es robuster willst: suche nach Ordnern "classification", "processing", ...
# (optional, aber nice)
if not (repo_root / "classification").exists():
    # fallback: nimm einen Level h√∂her, wenn du in classification/ bist
    if (repo_root.name.lower() == "classification"):
        repo_root = repo_root.parent

classification_dir = repo_root / "classification"
os.chdir(classification_dir)
print(f"Current working dir: {Path.cwd()}")

# --- Erwartete Dateien/Ordner repo-relativ ---
required_files = [
    "Input/Prompt_Subsidies_short_bullet.rtf",
    "Input/Prompt_Regulations_short_bullet.rtf",
    "Input/subsidies_classifications.RData",
    "Input/regulations_classifications.RData",
    "Input/functions_schemes.py",
]

required_folders = ["Output", "Logs"]

missing_files = [f for f in required_files if not (classification_dir / f).exists()]
missing_dirs = [d for d in required_folders if not (classification_dir / d).is_dir()]

# Optional: Output/Logs automatisch anlegen (empfohlen)
for d in required_folders:
    (classification_dir / d).mkdir(parents=True, exist_ok=True)

# Nach dem Anlegen nochmal pr√ºfen
missing_dirs = [d for d in required_folders if not (classification_dir / d).is_dir()]

if not missing_files and not missing_dirs:
    print("‚úÖ All required files/folders are present.")
else:
    print("‚ùå Missing:")
    if missing_files:
        print("  üìÑ Files:")
        for f in missing_files:
            print(f"   ‚¨ú {f}")
    if missing_dirs:
        print("  üìÅ Folders:")
        for d in missing_dirs:
            print(f"   ‚¨ú {d}")
    raise SystemExit("‚õîÔ∏è Classification aborted ‚Äì please add missing items.")
```

# Set Timestamp
```{python}
import os
from datetime import datetime as dt
timestamp = dt.now().strftime("%Y-%m-%d_%H-%M")
```

```{r}
timestamp <- format(Sys.time(), "%Y-%m-%d_%H-%M")
```

# Check packages
## Instalation
```{python}
import sys
import subprocess

# Erwartete Python-Version
expected_version = "3.11"

# Funktion zur √úberpr√ºfung der Python-Version
def check_python_version():
    current_version = sys.version.split(" ")[0]
    if not current_version.startswith(expected_version):
        print(f"Warnung: Aktuelle Python-Version ist {current_version}, erwartet wird {expected_version}.")
    else:
        print(f"Python-Version korrekt: {current_version}")

# Funktion zur Installation fehlender Pakete
def install_and_import(package):
    try:
        __import__(package)  # Versuche, das Paket zu importieren
    except ImportError:
        print(f"{package} wird installiert...")
        subprocess.check_call([sys.executable, "-m", "pip", "install", package])
    finally:
        globals()[package] = __import__(package)  # Importiere das Paket ins globale Namespace

# Liste der ben√∂tigten Pakete
required_packages = [
    "openai",
    "pandas",
    "pypandoc",
    "tiktoken",
    "keyring",
    "json",
    "time",
    "logging",
    "concurrent.futures",
    "tqdm",
    "multiprocessing",
    "openpyxl",
    "chardet",
    "pickle",
    "ace",
    "rpy2",
    "pyreadr",
    "pyarrow",
    "matplotlib",
    "hashlib",
    "uuid"
]

# √úberpr√ºfe und installiere alle Pakete
for package in required_packages:
    if package in ["concurrent.futures", "json", "time", "logging"]:  # Standardbibliotheken
        continue  # √úberspringe diese, da sie in der Standardbibliothek enthalten sind
    install_and_import(package)

# √úberpr√ºfe die Python-Version
check_python_version()

# Testausgabe, um sicherzustellen, dass alle Pakete geladen wurden
print("Alle erforderlichen Pakete sind geladen.")
```

## Import
```{python}
import openai
import pandas as pd
import pypandoc
import tiktoken
import keyring
import json
import time
import logging
import hashlib
import uuid
import pyreadr
from concurrent.futures import ThreadPoolExecutor
from tqdm import tqdm
from multiprocessing import cpu_count

print("Alle Pakete erfolgreich importiert!")
```

# GPT Tests
## Minimal Working Examples Neu
```{python}
import openai
import keyring
from openai import OpenAI

# Abruf des API-Schl√ºssels und der Organisation aus Keyring
API_KEY = keyring.get_password("openai", "api_key")

client = OpenAI(api_key=API_KEY)

# Chat-Historie f√ºr den Verlauf (optional)
chat_hist = []

try:
    # Test-Anfrage senden
    response = client.chat.completions.create(
        model="gpt-3.5-turbo",  # Modell ausw√§hlen
        messages=[
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": "Test query to check quota."}
        ],
        max_tokens=10  # Maximale Token-Anzahl f√ºr die Antwort
    )

    # Antwort ausgeben
    print("Dein Guthaben ist noch verf√ºgbar.")
    response_message = response.choices[0].message.content
    print(response_message)
except openai.RateLimitError:
    print("RateLimitError: Dein Guthaben ist wahrscheinlich aufgebraucht.")
except openai.AuthenticationError:
    print("AuthenticationError: Dein API-Schl√ºssel ist ung√ºltig oder fehlt.")
except openai.OpenAIError as e:
    print(f"Ein anderer OpenAI-Fehler ist aufgetreten: {e}")
except Exception as e:
    print(f"Ein unerwarteter Fehler ist aufgetreten: {e}")
```
## Modellabfrage
```{python}
from openai import OpenAI
import keyring
import os
# API-Schl√ºssel aus Keyring abrufen
API_KEY = keyring.get_password("openai", "api_key")

if API_KEY is None:
    raise ValueError("API-Schl√ºssel konnte nicht abgerufen werden. √úberpr√ºfen Sie keyring.")

# API-Schl√ºssel setzen
openai.api_key = API_KEY

client = OpenAI(api_key=API_KEY)

models = client.models.list()
print(models)

try:
    # Modelle abrufen und auflisten
    models = client.models.list()  # Direktes Abrufen der Modelle
    print("Verf√ºgbare Modelle:")
    for model in models:
        print(f"- {model.id} (Created: {model.created if hasattr(model, 'created') else 'N/A'})")
except openai.OpenAIError as e:
    print(f"Fehler beim Abrufen der Modelle: {e}")
except Exception as e:  # Korrigierter Exception-Name
    print(f"Ein unerwarteter Fehler ist aufgetreten: {e}")

```
## GPT4o Tester
```{python}
from openai import OpenAI
import openai
import keyring
import logging

# **Konfiguration**
API_KEY = keyring.get_password("openai", "api_key")

if API_KEY is None:
    raise ValueError("API-Schl√ºssel konnte nicht abgerufen werden. √úberpr√ºfen Sie Keyring.")

# Setze den API-Schl√ºssel
openai.api_key = API_KEY

client = OpenAI(api_key=API_KEY)

# Modellname definieren
MODEL_NAME = "gpt-4o-2024-11-20"
TEST_PROMPT = "Testing gpt-4o for compatibility."
LOG_FILE = "model_test.log"

# Logging konfigurieren
logging.basicConfig(level=logging.INFO, filename=LOG_FILE, filemode="a",
                    format="%(asctime)s - %(levelname)s - %(message)s")

# Funktion zum Testen des Modells
def test_model(model_name):
    try:
        logging.info(f"Teste Modell: {model_name}")
        response = client.chat.completions.create(
            model=model_name,
            messages=[
                {"role": "system", "content": "System test message."},
                {"role": "user", "content": TEST_PROMPT}
            ],
            max_tokens=10
        )
        response_message = response.choices[0].message.content
        logging.info(f"Antwort erhalten: {response_message}")
        print(f"Modell {model_name} erfolgreich getestet. Antwort: {response_message}")
    except openai.OpenAIError as e:  # Korrigierte Fehlerklasse
        logging.error(f"Fehler bei der Nutzung von {model_name}: {e}")
        print(f"Fehler bei der Nutzung von {model_name}: {e}")
    except Exception as e:
        logging.error(f"Unbekannter Fehler: {e}")
        print(f"Ein unbekannter Fehler ist aufgetreten: {e}")

# Test ausf√ºhren
test_model(MODEL_NAME)
```
## Function-Calling Check
```{python}
import pandas as pd
import os
import pyreadr
from functions_schemes import get_function_schema, get_function_name

print(get_function_name("subsidy"))
print(get_function_name("regulation"))
print(get_function_schema("subsidy")[0]["name"])
print(get_function_schema("regulation")[0]["name"])
```

# R-Loading Subsidies und Regulations
```{r}
library(reticulate)

# In R:
load("Input/subsidies_classifications.RData")  # oder readRDS("...")
ls()  # Was ist im Environment?
str(subsidies_classifications)  # Wie sieht das Objekt aus?
# √úbergebe das Objekt ins Python-Environment
py$df_subsidies = subsidies_classifications


load("Input/regulations_classifications.RData")  # oder readRDS("...")
ls()  # Was ist im Environment?
str(regulations_classifications)  # Wie sieht das Objekt aus?
# √úbergebe das Objekt ins Python-Environment
py$df_regulations = regulations_classifications

```


# 1. Subsidies

## Input Checks
## Dataframe Check Functions Calling
```{python}
import pandas as pd
import os
import pyreadr
import re
from functions_schemes import prepare_dataframe_for_function_calling, run_basic_quality_checks, save_dataframe_all_formats
print(df_subsidies.shape)
print(df_subsidies.columns)

df_subsidies_all = prepare_dataframe_for_function_calling(df_subsidies, "subsidy")

# üîπ Qualit√§tscheck
run_basic_quality_checks(
    df_subsidies_all,
    expected_rows=2190,
    expected_columns=64,
    policy_type="subsidy"
)

# -------------------------------
# üîß Funktion zur Textreinigung
# -------------------------------
def clean_text_column(df, column):
    if column not in df.columns:
        print(f"Spalte {column} nicht gefunden.")
        return df

    # Z√§hlvariablen
    num_double_spaces = 0
    num_newlines = 0
    num_tabs = 0
    rows_modified = 0
    total_modifications = 0
    max_length_before_cleaning = 0

    def clean_and_count(text):
        nonlocal num_double_spaces, num_newlines, num_tabs, rows_modified, total_modifications, max_length_before_cleaning
        if not isinstance(text, str):
            return text

        original = text
        max_length_before_cleaning = max(max_length_before_cleaning, len(original))

        # Einzelz√§hlung
        ds = len(re.findall(r"\s{2,}", original))
        nl = len(re.findall(r"\n+", original))
        tb = len(re.findall(r"\t", original))

        if ds + nl + tb > 0:
            rows_modified += 1
            num_double_spaces += ds
            num_newlines += nl
            num_tabs += tb
            total_modifications += (ds + nl + tb)

        # Bereinigung
        text = original.replace('\t', ' ')
        text = re.sub(r'\s+', ' ', text).strip()
        return text

    df[column] = df[column].apply(clean_and_count)

    # Ausgabe
    print(f"\nüßπ Reinigungsergebnisse f√ºr Spalte '{column}':")
    print(f"  ‚ñ∏ Ge√§nderte Zeilen: {rows_modified}")
    print(f"  ‚ñ∏ Gesamte √Ñnderungen: {total_modifications}")
    print(f"     ‚Ü™ doppelte Leerzeichen: {num_double_spaces}")
    print(f"     ‚Ü™ Zeilenumbr√ºche: {num_newlines}")
    print(f"     ‚Ü™ Tabs: {num_tabs}")
    if rows_modified > 0:
        print(f"  ‚ñ∏ Durchschnittliche √Ñnderungen pro betroffener Zeile: {total_modifications / rows_modified:.2f}")
    print(f"  ‚ñ∏ Maximale Textl√§nge vor Reinigung: {max_length_before_cleaning} Zeichen\n")

    return df

# --------------------------------
# üßπ Reinigung vor Analyse
# --------------------------------
df_subsidies_all = clean_text_column(df_subsidies_all, "Text")

# --------------------------------
# üìä Textl√§ngen-Analyse nach Bereinigung
# --------------------------------
if "Text" in df_subsidies_all.columns:
    lengths = df_subsidies_all["Text"].dropna().apply(lambda x: len(x) if isinstance(x, str) else 0)
    num_missing_or_empty = df_subsidies_all["Text"].apply(
        lambda x: not isinstance(x, str) or len(x.strip()) == 0
    ).sum()

    print("\nüìä Zeichenzahlen-Analyse der Spalte 'Text':")
    print(f"  ‚ñ∏ Anzahl g√ºltiger Texte: {len(lengths)}")
    print(f"  ‚ñ∏ Durchschnittliche Zeichenzahl: {lengths.mean():.2f}")
    print(f"  ‚ñ∏ Median der Zeichenzahl:       {lengths.median():.2f}")
    print(f"  ‚ñ∏ Maximale Zeichenzahl:         {lengths.max()}")
    print(f"  ‚ñ∏ Minimale Zeichenzahl:         {lengths.min()}")
    print(f"  ‚ñ∏ Leere oder fehlende Eintr√§ge: {num_missing_or_empty}\n")

# --------------------------------
# ‚úÇÔ∏è K√ºrzungs-Check & Truncation
# --------------------------------
if "Text" in df_subsidies_all.columns:
    num_texts_cut = df_subsidies_all["Text"].apply(lambda x: isinstance(x, str) and len(x) > 7000).sum()
    print(f"‚úÇÔ∏è Texte √ºber 7000 Zeichen: {num_texts_cut}")
    long_texts = df_subsidies_all[df_subsidies_all["Text"].apply(lambda x: isinstance(x, str) and len(x) > 7000)]
    df_subsidies_all["Text"] = df_subsidies_all["Text"].apply(lambda x: x[:7000] if isinstance(x, str) else x)

if "Policy_Name" in df_subsidies_all.columns:
    num_names_cut = df_subsidies_all["Policy_Name"].apply(lambda x: isinstance(x, str) and len(x) > 2000).sum()
    print(f"‚úÇÔ∏è Policy-Namen √ºber 2000 Zeichen: {num_names_cut}")
    long_names = df_subsidies_all[df_subsidies_all["Policy_Name"].apply(lambda x: isinstance(x, str) and len(x) > 2000)]
    df_subsidies_all["Policy_Name"] = df_subsidies_all["Policy_Name"].apply(lambda x: x[:2000] if isinstance(x, str) else x)

# --------------------------------
# üíæ Optional: Export der langen Originaleintr√§ge
# --------------------------------
if not long_texts.empty:
    long_texts.to_excel("long_text_entries_subsidies.xlsx", index=False)
if not long_names.empty:
    long_names.to_excel("long_policy_name_entries_subsidies.xlsx", index=False)

# --------------------------------
# üíæ Speichern des bearbeiteten DataFrames
# --------------------------------
save_dataframe_all_formats(df_subsidies_all, "subsidies_updated_all")

print("üîÄ Zuf√§llige Neuordnung der Beobachtungen...")
df_subsidies_all = df_subsidies_all.sample(frac=1, random_state=31).reset_index(drop=True)
```

# 1.2 Subsidies Short/Bullet-point

### Prompt Bereinigung Check und L√§nge, Tokenberechnung und Kostenberechnung
```{python}
# Tokenberechnung f√ºr Prompt + DataFrame (Subsidies/Regulations)

import pandas as pd
import tiktoken
import pypandoc
import re
import os

MODEL_NAME = "gpt-4o-2024-11-20"
prompt_tag = "subsidies_short"

def load_and_clean_prompt(rtf_file):
    """
    Liest den Prompt aus einer RTF-Datei und bereinigt ihn.
    """
    try:
        prompt_plain = pypandoc.convert_file(rtf_file, 'plain')
        clean_prompt = (
            prompt_plain
            .replace("{{", "{")
            .replace("}}", "}")
            .replace("\\", "")
            .replace("\u00A0", " ")     # gesch√ºtzte Leerzeichen ‚Üí normale Leerzeichen
            .replace("‚Äô", "'")          # typografische Apostrophe
            .replace("‚Äú", '"').replace("‚Äù", '"')  # typografische Anf√ºhrungszeichen
            .replace("``", '"')         # doppelte Backticks zu normalen Quotes
        )
        clean_prompt = re.sub(r"\s+", " ", clean_prompt).strip()  # Mehrfachleerzeichen reduzieren
        return clean_prompt
    except Exception as e:
        print(f"Fehler beim Bereinigen des Prompts: {e}")
        raise

def calculate_total_tokens_for_prompt_and_dataframe(model_name, prompt_file, dataframe_file):
    """
    Berechnet die Gesamtanzahl der Tokens f√ºr Prompt, Daten & Completion.
    Gibt zus√§tzlich eine Kostensch√§tzung in $ aus.
    """
    # Prompt laden
    prompt = load_and_clean_prompt(prompt_file)
    encoder = tiktoken.encoding_for_model(model_name)
    prompt_token_count = len(encoder.encode(prompt))

    # Daten einlesen
    df = pd.read_excel(dataframe_file, engine="openpyxl")
    df.columns = df.columns.str.replace(" ", "_")

    # Tokens f√ºr Beobachtungen berechnen
    row_token_count = 0
    for row in df.itertuples(index=False):
        row_dict = row._asdict()
        combined_row = " ".join([str(v) for v in row_dict.values() if pd.notna(v)])
        row_token_count += len(encoder.encode(combined_row))

    # GPT-4o Preise (April 2024)
    input_price_per_1k = 0.005
    output_price_per_1k = 0.015
    estimated_completion_tokens_per_row = 300
    
    prompt_token_count_total = prompt_token_count * len(df)
    estimated_total_completion_tokens = 250 * len(df)  # realistischere Sch√§tzung
    total_tokens = prompt_token_count_total + row_token_count + estimated_total_completion_tokens

    cost_estimate = (
        (prompt_token_count + row_token_count) / 1000 * input_price_per_1k +
        estimated_total_completion_tokens / 1000 * output_price_per_1k
    )
    cost_estimate = round(cost_estimate, 4)

    print(f"\nüìä Token-Berechnung f√ºr '{dataframe_file}' mit Modell '{model_name}':")
    print(f"üîπ Prompt-Token-Anzahl:     {prompt_token_count}")
    print(f"üîπ DataFrame-Token-Anzahl:  {row_token_count}")
    print(f"üîπ Gesch√§tzte Completion:   {estimated_total_completion_tokens}")
    print(f"üîπ Gesamttokens:            {total_tokens}")
    print(f"üí∞ Gesch√§tzte Kosten:       ${cost_estimate:.4f}")

    return {
        "prompt_tokens": prompt_token_count,
        "dataframe_tokens": row_token_count,
        "completion_tokens_est": estimated_total_completion_tokens,
        "total_tokens": total_tokens,
        "estimated_cost_usd": cost_estimate
    }

# Beispielausf√ºhrung
if __name__ == "__main__":
    PROMPT_FILE = "Input/Prompt_Subsidies_short_bullet.rtf"
    INPUT_FILE = "Output/cleaned_subsidies_updated_all.xlsx"

    calculate_total_tokens_for_prompt_and_dataframe(
        model_name=MODEL_NAME,
        prompt_file=PROMPT_FILE,
        dataframe_file=INPUT_FILE
    )
```

### Prompt anzeigen lassen
```{python}
import pypandoc

# üîß Pfad zur Prompt-Datei
prompt_file = "Input/Prompt_Subsidies_short_bullet.rtf"

# üßπ Funktion zum Bereinigen
def load_and_clean_prompt(rtf_file):
    try:
        prompt_plain = pypandoc.convert_file(rtf_file, 'plain')
        clean_prompt = (
            prompt_plain
            .replace("{{", "{")
            .replace("}}", "}")
            .replace("\\", "")
            .replace("\u00A0", " ")     # gesch√ºtzte Leerzeichen ‚Üí normale Leerzeichen
            .replace("‚Äô", "'")          # typografische Apostrophe
            .replace("‚Äú", '"').replace("‚Äù", '"')  # typografische Anf√ºhrungszeichen
            .replace("``", '"')         # doppelte Backticks zu normalen Quotes
        )
        clean_prompt = re.sub(r"\s+", " ", clean_prompt).strip()  # Mehrfachleerzeichen reduzieren
        return clean_prompt
    except Exception as e:
        print(f"Fehler beim Bereinigen des Prompts: {e}")
        return None

# üì§ Prompt anzeigen
cleaned_prompt = load_and_clean_prompt(prompt_file)

if cleaned_prompt:
    print("üìù Vollst√§ndiger bereinigter Prompt:\n")
    print(cleaned_prompt)
```


# 2. Regulations

## Input Checks
## Dataframe Check Functions Calling

```{python}
import pandas as pd
import os
import pyreadr
import re
from functions_schemes import prepare_dataframe_for_function_calling, run_basic_quality_checks, save_dataframe_all_formats

print(df_regulations.shape)
print(df_regulations.columns)

df_regulations_all = prepare_dataframe_for_function_calling(df_regulations, "regulation")

# üîπ Qualit√§tscheck
run_basic_quality_checks(
    df_regulations_all,
    expected_rows=1214,
    expected_columns=64,
    policy_type="regulation"
)

# -------------------------------
# üîß Funktion zur Textreinigung
# -------------------------------
def clean_text_column(df, column):
    if column not in df.columns:
        print(f"Spalte {column} nicht gefunden.")
        return df

    # Z√§hlvariablen
    num_double_spaces = 0
    num_newlines = 0
    num_tabs = 0
    rows_modified = 0
    total_modifications = 0
    max_length_before_cleaning = 0

    def clean_and_count(text):
        nonlocal num_double_spaces, num_newlines, num_tabs, rows_modified, total_modifications, max_length_before_cleaning
        if not isinstance(text, str):
            return text

        original = text
        max_length_before_cleaning = max(max_length_before_cleaning, len(original))

        # Einzelz√§hlung
        ds = len(re.findall(r"\s{2,}", original))
        nl = len(re.findall(r"\n+", original))
        tb = len(re.findall(r"\t", original))

        if ds + nl + tb > 0:
            rows_modified += 1
            num_double_spaces += ds
            num_newlines += nl
            num_tabs += tb
            total_modifications += (ds + nl + tb)

        # Bereinigung
        text = original.replace('\t', ' ')
        text = re.sub(r'\s+', ' ', text).strip()
        return text

    df[column] = df[column].apply(clean_and_count)

    # Ausgabe
    print(f"\nüßπ Reinigungsergebnisse f√ºr Spalte '{column}':")
    print(f"  ‚ñ∏ Ge√§nderte Zeilen: {rows_modified}")
    print(f"  ‚ñ∏ Gesamte √Ñnderungen: {total_modifications}")
    print(f"     ‚Ü™ doppelte Leerzeichen: {num_double_spaces}")
    print(f"     ‚Ü™ Zeilenumbr√ºche: {num_newlines}")
    print(f"     ‚Ü™ Tabs: {num_tabs}")
    if rows_modified > 0:
        print(f"  ‚ñ∏ Durchschnittliche √Ñnderungen pro betroffener Zeile: {total_modifications / rows_modified:.2f}")
    print(f"  ‚ñ∏ Maximale Textl√§nge vor Reinigung: {max_length_before_cleaning} Zeichen\n")

    return df

# --------------------------------
# üßπ Reinigung vor Analyse
# --------------------------------

df_regulations_all = clean_text_column(df_regulations_all, "Text")

# --------------------------------
# üìä Textl√§ngen-Analyse nach Bereinigung
# --------------------------------
if "Text" in df_regulations_all.columns:
    lengths = df_regulations_all["Text"].dropna().apply(lambda x: len(x) if isinstance(x, str) else 0)
    num_missing_or_empty = df_regulations_all["Text"].apply(
        lambda x: not isinstance(x, str) or len(x.strip()) == 0
    ).sum()

    print("\nüìä Zeichenzahlen-Analyse der Spalte 'Text':")
    print(f"  ‚ñ∏ Anzahl g√ºltiger Texte: {len(lengths)}")
    print(f"  ‚ñ∏ Durchschnittliche Zeichenzahl: {lengths.mean():.2f}")
    print(f"  ‚ñ∏ Median der Zeichenzahl:       {lengths.median():.2f}")
    print(f"  ‚ñ∏ Maximale Zeichenzahl:         {lengths.max()}")
    print(f"  ‚ñ∏ Minimale Zeichenzahl:         {lengths.min()}")
    print(f"  ‚ñ∏ Leere oder fehlende Eintr√§ge: {num_missing_or_empty}\n")

# --------------------------------
# ‚úÇÔ∏è K√ºrzungs-Check & Truncation
# --------------------------------
if "Text" in df_regulations_all.columns:
    num_texts_cut = df_regulations_all["Text"].apply(lambda x: isinstance(x, str) and len(x) > 7000).sum()
    print(f"‚úÇÔ∏è Texte √ºber 7000 Zeichen: {num_texts_cut}")
    long_texts = df_regulations_all[df_regulations_all["Text"].apply(lambda x: isinstance(x, str) and len(x) > 7000)]
    df_regulations_all["Text"] = df_regulations_all["Text"].apply(lambda x: x[:7000] if isinstance(x, str) else x)

if "Policy_Name" in df_regulations_all.columns:
    num_names_cut = df_regulations_all["Policy_Name"].apply(lambda x: isinstance(x, str) and len(x) > 2000).sum()
    print(f"‚úÇÔ∏è Policy-Namen √ºber 2000 Zeichen: {num_names_cut}")
    long_names = df_regulations_all[df_regulations_all["Policy_Name"].apply(lambda x: isinstance(x, str) and len(x) > 2000)]
    df_regulations_all["Policy_Name"] = df_regulations_all["Policy_Name"].apply(lambda x: x[:2000] if isinstance(x, str) else x)

# --------------------------------
# üíæ Optional: Export der langen Originaleintr√§ge
# --------------------------------
if not long_texts.empty:
    long_texts.to_excel("long_text_entries_regulations.xlsx", index=False)
if not long_names.empty:
    long_names.to_excel("long_policy_name_entries_regulations.xlsx", index=False)

# --------------------------------
# üíæ Speichern des bearbeiteten DataFrames
# --------------------------------
save_dataframe_all_formats(df_regulations_all, "regulations_updated_all")

print("üîÄ Zuf√§llige Neuordnung der Beobachtungen...")
df_regulations_all = df_regulations_all.sample(frac=1, random_state=73).reset_index(drop=True)
```

# 2.2 Regulations Short/Bullet-point

### Prompt Bereinigung Check und L√§nge, Tokenberechnung und Kostenberechnung
```{python}
# Tokenberechnung f√ºr Prompt + DataFrame (Subsidies/Regulations)

import pandas as pd
import tiktoken
import pypandoc
import re
import os

MODEL_NAME = "gpt-4o-2024-11-20"
prompt_tag = "regulations_short"

def load_and_clean_prompt(rtf_file):
    """
    Liest den Prompt aus einer RTF-Datei und bereinigt ihn.
    """
    try:
        prompt_plain = pypandoc.convert_file(rtf_file, 'plain')
        clean_prompt = (
            prompt_plain
            .replace("{{", "{")
            .replace("}}", "}")
            .replace("\\", "")
            .replace("\u00A0", " ")     # gesch√ºtzte Leerzeichen ‚Üí normale Leerzeichen
            .replace("‚Äô", "'")          # typografische Apostrophe
            .replace("‚Äú", '"').replace("‚Äù", '"')  # typografische Anf√ºhrungszeichen
            .replace("``", '"')         # doppelte Backticks zu normalen Quotes
        )
        clean_prompt = re.sub(r"\s+", " ", clean_prompt).strip()  # Mehrfachleerzeichen reduzieren
        return clean_prompt
    except Exception as e:
        print(f"Fehler beim Bereinigen des Prompts: {e}")
        raise

def calculate_total_tokens_for_prompt_and_dataframe(model_name, prompt_file, dataframe_file):
    """
    Berechnet die Gesamtanzahl der Tokens f√ºr Prompt, Daten & Completion.
    Gibt zus√§tzlich eine Kostensch√§tzung in $ aus.
    """
    # Prompt laden
    prompt = load_and_clean_prompt(prompt_file)
    encoder = tiktoken.encoding_for_model(model_name)
    prompt_token_count = len(encoder.encode(prompt))

    # Daten einlesen
    df = pd.read_excel(dataframe_file, engine="openpyxl")
    df.columns = df.columns.str.replace(" ", "_")

    # Tokens f√ºr Beobachtungen berechnen
    row_token_count = 0
    for row in df.itertuples(index=False):
        row_dict = row._asdict()
        combined_row = " ".join([str(v) for v in row_dict.values() if pd.notna(v)])
        row_token_count += len(encoder.encode(combined_row))

    # GPT-4o Preise (April 2024)
    input_price_per_1k = 0.005
    output_price_per_1k = 0.015
    estimated_completion_tokens_per_row = 300
    
    prompt_token_count_total = prompt_token_count * len(df)
    estimated_total_completion_tokens = 250 * len(df)  # realistischere Sch√§tzung
    total_tokens = prompt_token_count_total + row_token_count + estimated_total_completion_tokens

    cost_estimate = (
        (prompt_token_count + row_token_count) / 1000 * input_price_per_1k +
        estimated_total_completion_tokens / 1000 * output_price_per_1k
    )
    cost_estimate = round(cost_estimate, 4)

    print(f"\nüìä Token-Berechnung f√ºr '{dataframe_file}' mit Modell '{model_name}':")
    print(f"üîπ Prompt-Token-Anzahl:     {prompt_token_count}")
    print(f"üîπ DataFrame-Token-Anzahl:  {row_token_count}")
    print(f"üîπ Gesch√§tzte Completion:   {estimated_total_completion_tokens}")
    print(f"üîπ Gesamttokens:            {total_tokens}")
    print(f"üí∞ Gesch√§tzte Kosten:       ${cost_estimate:.4f}")


# Beispielausf√ºhrung
if __name__ == "__main__":
    PROMPT_FILE = "Input/Prompt_Regulations_short_bullet.rtf"
    INPUT_FILE = "Output/cleaned_regulations_updated_all.xlsx"

    calculate_total_tokens_for_prompt_and_dataframe(
        model_name=MODEL_NAME,
        prompt_file=PROMPT_FILE,
        dataframe_file=INPUT_FILE
    )
```

### Prompt anzeigen lassen
```{python}
import pypandoc

# üîß Pfad zur Prompt-Datei
prompt_file = "Input/Prompt_Regulations_short_bullet.rtf"

# üßπ Funktion zum Bereinigen
def load_and_clean_prompt(rtf_file):
    try:
        prompt_plain = pypandoc.convert_file(rtf_file, 'plain')
        clean_prompt = (
            prompt_plain
            .replace("{{", "{")
            .replace("}}", "}")
            .replace("\\", "")
            .replace("\u00A0", " ")     # gesch√ºtzte Leerzeichen ‚Üí normale Leerzeichen
            .replace("‚Äô", "'")          # typografische Apostrophe
            .replace("‚Äú", '"').replace("‚Äù", '"')  # typografische Anf√ºhrungszeichen
            .replace("``", '"')         # doppelte Backticks zu normalen Quotes
        )
        clean_prompt = re.sub(r"\s+", " ", clean_prompt).strip()  # Mehrfachleerzeichen reduzieren
        return clean_prompt
    except Exception as e:
        print(f"Fehler beim Bereinigen des Prompts: {e}")
        return None

# üì§ Prompt anzeigen
cleaned_prompt = load_and_clean_prompt(prompt_file)

if cleaned_prompt:
    print("üìù Vollst√§ndiger bereinigter Prompt:\n")
    print(cleaned_prompt)
```


# Subsidies Running Klassifikation 
## Subsidies Short Einfach
```{python}
import os
import json
import time
import csv
import pandas as pd
import pypandoc
import keyring
import openai
import tiktoken
import hashlib
import uuid
from tqdm import tqdm
from concurrent.futures import ThreadPoolExecutor
from datetime import datetime as dt
from functions_schemes import get_function_schema, get_function_name
import re
from openai import OpenAIError, RateLimitError
# === Neue Wrapper-Funktion f√ºr Trial mit Wiederholungen (beide Prompts) ===
import random
from pathlib import Path
from datetime import datetime
from classification_trial_evaluator_v5 import check_consumer_orientation_deviation
from evaluate_last_trials import export_trial_outputs

# === Konfiguration ===
prompt_tag = "subsidies"
MODEL_NAME = "gpt-4o"
PROMPT_FILE = "Input/Prompt_Subsidies_short_bullet.rtf"
POLICY_TYPE = "subsidy"
LOG_DIR = "Logs"
os.makedirs(LOG_DIR, exist_ok=True)
USE_CACHE = False  # Toggle fuer Testzwecke

# === OpenAI API Key ===
API_KEY = keyring.get_password("openai", "api_key")
if not API_KEY:
    raise RuntimeError("‚ùå Kein API-Key gefunden.")
openai.api_key = API_KEY
print("üîê API-Key geladen")

# Tokenlimit f√ºr Prompt + Completion
MODEL_TOTAL_LIMIT = {
    "gpt-4": 8192,
    "gpt-4o": 128000,  # korrekt f√ºr Berechnung
    "gpt-4o-2024-11-20": 128000, 
}
# Maximaler Antwort-Output
MODEL_COMPLETION_LIMIT = {
    "gpt-4": 4096,
    "gpt-4o": 4096, 
    "gpt-4o-2024-11-20": 4096,# ‚úÖ nur das ist max_tokens (Antwortl√§nge!)
}

# === Funktionen ===
def load_and_clean_prompt(rtf_file):
    try:
        # 1Ô∏è‚É£ RTF ‚Üí Plain Text
        prompt_plain = pypandoc.convert_file(rtf_file, 'plain')

        # 2Ô∏è‚É£ Bereinigen
        clean_prompt = (
            prompt_plain
            .replace("{{", "{")
            .replace("}}", "}")
            .replace("\\", "")
            .replace("\u00A0", " ")  # gesch√ºtztes Leerzeichen
            .replace("‚Äô", "'")       # typografischer Apostroph
            .replace("‚Äú", '"').replace("‚Äù", '"')  # typografische Quotes
            .replace("``", '"')      # doppelte Backticks
        )
        clean_prompt = re.sub(r"\s+", " ", clean_prompt).strip()

        # 3Ô∏è‚É£ Platzhalter-Pr√ºfung (nach dem Cleaning!)
        if "{Policy_ID}" not in clean_prompt:
            print("‚ö†Ô∏è Warnung: Policy_ID wird nicht im Prompt verwendet!")
        if not all(var in clean_prompt for var in ["{Country}", "{Year}", "{Policy_Name}"]):
            print("‚ö†Ô∏è Achtung: Country, Year oder Policy_Name sind nicht im Prompt enthalten!")
        if "{Text}" not in clean_prompt:
            print("‚ö†Ô∏è Die Policy-Beschreibung (Text) ist nicht im Prompt enthalten!")
        return clean_prompt

    except Exception as e:
        print(f"‚ùå Fehler beim Laden oder Bereinigen des Prompts: {e}")
        return None

def calculate_remaining_tokens(prompt_text):
    tokenizer = tiktoken.encoding_for_model(MODEL_NAME)
    prompt_tokens = len(tokenizer.encode(prompt_text))
    return MODEL_TOTAL_LIMIT[MODEL_NAME] - prompt_tokens

def load_cache(cache_file):
    if os.path.exists(cache_file):
        try:
            with open(cache_file, "r") as f:
                return json.load(f)
        except json.JSONDecodeError:
            return {}
    return {}

def save_cache(cache, cache_file):
    with open(cache_file, "w") as f:
        json.dump(cache, f)

def append_to_log(row_id, tokens, attempts, log_path, error=None, success=True):
    log_entry = {
        "id_countryiso_year_url": row_id,
        "Tokens": tokens,
        "Attempts": attempts,
        "Success": success,
        "Error_Message": error or "",
        "Timestamp": dt.now().isoformat()
    }
    file_exists = os.path.exists(log_path)
    with open(log_path, "a", newline="", encoding="utf-8") as logfile:
        writer = csv.DictWriter(logfile, fieldnames=log_entry.keys(), delimiter=";")
        if not file_exists:
            writer.writeheader()
        writer.writerow(log_entry)

def classify_with_function_calling(row, prompt, max_output_tokens, log_path, cache_file,trial_id, retries=10):
    row_dict = row._asdict()

    # ID vorbereiten
    if "id_countryiso_year_url" not in row_dict:
        raise ValueError("‚ùå 'id_countryiso_year_url' fehlt.")
    row_id = str(int(float(row_dict["id_countryiso_year_url"])))
    row_dict["Policy_ID"] = row_id
    row_dict["id_countryiso_year_url"] = row_id

    print("üìÑ Policy Text:", row_dict.get("Text", "")[:300], "\n---")

    # Prompt f√ºllen
    try:
        formatted_prompt = prompt.format(**row_dict)
    except KeyError as e:
        print(f"‚ùå Fehlender Schl√ºssel im Prompt: {e}")
        return {"Error": f"Missing key: {e}"}

    print(f"\nüßæ Prompt-Vorschau f√ºr {row_id} (Versuch 1):\n{formatted_prompt[:500]}...\n---")

    cache = load_cache(cache_file)
    if USE_CACHE and row_id in cache:
        print(f"üîÅ √úberspringe {row_id} ‚Äì aus Cache geladen.")
        return cache[row_id]

    for attempt in range(1, retries + 1):
        try:
            print(f"üîé Klassifiziere {row_id} (Versuch {attempt}/{retries})")
            
            tokenizer = tiktoken.encoding_for_model(MODEL_NAME)
            prompt_tokens = len(tokenizer.encode(formatted_prompt))
            max_tokens_dynamic = min(
            MODEL_COMPLETION_LIMIT[MODEL_NAME],
            MODEL_TOTAL_LIMIT[MODEL_NAME] - prompt_tokens - 200
        )
            
            # ‚úÖ Debug-Ausgabe hier:
            tokenizer = tiktoken.encoding_for_model(MODEL_NAME)
            prompt_length = len(tokenizer.encode(formatted_prompt))
            print(f"üìè Promptl√§nge: {prompt_length} Tokens ‚Äì Verf√ºgbar f√ºr Output: {max_tokens_dynamic}")

            response = openai.chat.completions.create(
                model=MODEL_NAME,
                messages=[{"role": "user", "content": formatted_prompt}],
                functions=get_function_schema(POLICY_TYPE),
                function_call={"name": get_function_name(POLICY_TYPE)},
                temperature=0.0,
                top_p=1.0,
                frequency_penalty=0.0,
                presence_penalty=0.0,
                max_tokens=max_tokens_dynamic
            )

            func_response = response.choices[0].message.function_call
            if not func_response:
                raise ValueError("Keine function_call-Antwort erhalten.")

            result = json.loads(func_response.arguments)
            result["id_countryiso_year_url"] = row_id
            result["Country"] = row_dict.get("Country")
            result["Year"] = row_dict.get("Year")
            result["Policy_Name"] = row_dict.get("Policy_Name")
            result["Tokens"] = response.usage.total_tokens
            
             # üîê Eindeutiger Hash des Originaltexts
            result["Input_Policy_Hash"] = hashlib.md5(row_dict["Text"].encode("utf-8")).hexdigest()
            # GPT usage-Metadaten
            usage = response.usage
            result["Prompt_Tokens"] = usage.prompt_tokens
            result["Completion_Tokens"] = usage.completion_tokens
            result["Total_Tokens"] = usage.total_tokens
            # Zusatzinfos aus function_call
            result["Function_Name"] = func_response.name
            result["Function_Arguments_JSON"] = func_response.arguments  # optional f√ºr Debugging
            # Tokenbudget & Prompt-Metriken
            result["Prompt_Length_Tokens"] = prompt_tokens
            result["Max_Output_Tokens"] = max_tokens_dynamic
            # Weitere Klassifikations-Metadaten
            result["Retry_Count"] = attempt
            result["Used_Cache"] = USE_CACHE and row_id in cache
            # Allgemeine Metadaten (schon teilweise enthalten)
            result["Model_Name"] = MODEL_NAME
            result["Prompt_Version"] = prompt_tag  # z.‚ÄØB. "subsidies_short"
            result["Timestamp"] = dt.now().isoformat()
            result["Trial_ID"] = trial_id
            result["Run_UUID"] = str(uuid.uuid4())
            
            append_to_log(row_id, result["Tokens"], attempt, log_path)
            cache[row_id] = result
            save_cache(cache, cache_file)
            return result

        except RateLimitError as e:
            retry_after = 10
            match = re.search(r"try again in ([\\d.]+)s", str(e))
            if match:
                retry_after = float(match.group(1))
            print(f"‚è≥ Rate Limit erreicht. Warte {retry_after:.1f} Sekunden...")
            time.sleep(retry_after)

        except Exception as e:
            print(f"‚ö†Ô∏è Fehler bei {row_id} (Versuch {attempt}): {e}")
            append_to_log(row_id, 0, attempt, log_path=log_path, error=str(e), success=False)
            time.sleep(2 + attempt)

    final_error = f"‚ùå Maximale Anzahl Versuche erreicht f√ºr {row_id}"
    append_to_log(row_id, 0, retries, log_path=log_path, error=final_error, success=False)
    return {"Error": final_error}

def classify_observations(data, output_path, log_path,cache_file, trial_id ):
    print("üìÑ Verwende √ºbergebenen DataFrame.")
    print(f"üìä Anzahl geladener Beobachtungen: {len(data)}")
    print("üìä Datensatz geladen:", data.shape)
    print("üÜî Beobachtungen, die klassifiziert werden:")
    print(data["id_countryiso_year_url"].tolist())

    data["id_countryiso_year_url"] = data["id_countryiso_year_url"].apply(lambda x: str(int(x)) if pd.notna(x) else None)

    if data["id_countryiso_year_url"].isna().sum() > 0:
        raise ValueError("‚ùå Fehlende IDs.")
    if data["id_countryiso_year_url"].duplicated().sum() > 0:
        raise ValueError("‚ùå Doppelte IDs.")

    prompt_template = load_and_clean_prompt(PROMPT_FILE)
    max_output_tokens = calculate_remaining_tokens(prompt_template) - 100
    print(f"üìè Prompt-Token-Budget: {max_output_tokens} Tokens verf√ºgbar")
    print("üöÄ Starte Klassifikation...\n")

    results = []
    with ThreadPoolExecutor(max_workers=4) as executor:
      for result in tqdm(
          executor.map(lambda args: classify_with_function_calling(*args),
                       ((row, prompt_template, max_output_tokens, log_path, cache_file, trial_id)
                       for row in data.itertuples(index=False))),
        total=len(data), desc="üîÑ Klassifikation l√§uft"
    ):
        results.append(result)

    result_df = pd.DataFrame(results)
    result_df.to_csv(output_path, index=False, float_format='%.0f')
    print(f"\n‚úÖ Ergebnisse gespeichert unter: {output_path}")
    result_df.to_excel(output_path.replace(".csv", ".xlsx"), index=False)
    result_df.to_pickle(output_path.replace(".csv", ".pkl"))
    print("üìÅ Ausgabe zus√§tzlich als .xlsx und .pkl gespeichert")
    return result_df

# === Vergleich & Fehleranalyse ============================
def post_classification_analysis(result_df, output_path, log_path, cache_file, timestamp):
    print("\nüßæ Starte Analyse der erfolgreichen & fehlerhaften Klassifikationen...")

    # üîÅ IDs vereinheitlichen
    result_df["id_countryiso_year_url"] = result_df["id_countryiso_year_url"].astype(str)

    if not os.path.exists(log_path):
        print("‚ö†Ô∏è Kein Logfile gefunden. Fehleranalyse nicht m√∂glich.")
        return

    # üîç Logfile laden
    log_df = pd.read_csv(log_path, sep=";")
    log_df.columns = log_df.columns.str.strip()  # Whitespace-Fehler vermeiden
    log_df["id_countryiso_year_url"] = log_df["id_countryiso_year_url"].astype(str)
    print(f"üìã Erkannte Spalten im Logfile: {log_df.columns.tolist()}")
    log_df["Attempts"] = pd.to_numeric(log_df["Attempts"], errors="coerce").fillna(0).astype(int)
    log_df["Tokens"] = pd.to_numeric(log_df["Tokens"], errors="coerce").fillna(0).astype(int)
    
    # ‚ö†Ô∏è Sicherheitscheck: Gr√∂√üe der Ergebnisdatei vs. Logdatei
    if len(result_df) < len(log_df):
        print("‚ö†Ô∏è Ergebnisdatei enth√§lt weniger Zeilen als Logdatei. Bitte pr√ºfen, ob das die aktuelle Klassifikationsdatei ist.")
        
    if "Success" not in log_df.columns:
        print("‚ùå Die Spalte 'Success' fehlt im Logfile. Analyse wird abgebrochen.")
        return

    # üîó Merge zur Identifikation erfolgreicher Klassifikationen
    merged_df = result_df.merge(
        log_df[["id_countryiso_year_url", "Success", "Error_Message", "Attempts"]],
        on="id_countryiso_year_url", how="left"
    )

    # ‚úÖ Erfolgreiche F√§lle (Success == True)
    result_df_successful = merged_df[merged_df["Success"] == True]
    success_path = output_path.replace(".csv", "_success_only.csv")
    result_df_successful.to_csv(success_path, index=False)
    print(f"üìÇ Erfolgreiche Ergebnisse gespeichert unter: {success_path}")

    # üî¢ √úbersicht
    all_ids = set(log_df["id_countryiso_year_url"])
    success_ids = set(result_df_successful["id_countryiso_year_url"])
    failed_ids = all_ids - success_ids

    print(f"üî¢ Gesamt: {len(all_ids)}")
    print(f"‚úÖ Erfolgreich klassifiziert: {len(success_ids)}")
    print(f"‚ùå Fehlgeschlagen: {len(failed_ids)}")

    # üö´ Fehleranalyse
    failed_entries = log_df[log_df["id_countryiso_year_url"].isin(failed_ids)]
    error_summary = failed_entries["Error_Message"].value_counts()

    print("\nüö´ Gr√ºnde f√ºr Fehlversuche:")
    print(error_summary if not error_summary.empty else "‚úÖ Keine Fehler protokolliert.")

    # üîé Nicht korrekt identifizierte Policies
    if "Correctly_Identified" in result_df.columns:
        not_identified = result_df[result_df["Correctly_Identified"] == False]
        print(f"\nüö´ Nicht korrekt identifizierte Policies: {len(not_identified)}")
        if not not_identified.empty:
            not_identified.to_csv( f"Output/not_identified_policies_{prompt_tag}_{timestamp}.csv", index=False)
            print(f"üìÇ Nicht erkannte Policies gespeichert unter: Output/not_identified_policies_{timestamp}.csv")

    # ‚è≥ Max. Anzahl Versuche erreicht?
    max_attempts = failed_entries[failed_entries["Attempts"] >= 10]
    print(f"\n‚è≥ F√§lle mit max. Versuchen (>=5): {len(max_attempts)}")

    # ‚ö†Ô∏è Logfile-IDs nicht im Ergebnis-CSV?
    result_ids = set(result_df["id_countryiso_year_url"])
    missing_in_result = all_ids - result_ids
    if missing_in_result:
        print(f"\n‚ö†Ô∏è IDs im Logfile, aber nicht im Ergebnis-CSV: {len(missing_in_result)}")
        print(sorted(list(missing_in_result)))

    # üíæ Fehlerdetails exportieren
    failed_entries.to_csv( f"Output/failed_classifications_detailed_{prompt_tag}_{timestamp}.csv", index=False)
    print("üìÅ Fehlerdetails gespeichert unter: Output/failed_classifications_detailed.csv")
# === Konfigurierbare Trial-Funktion ===
def run_trials(
    data,
    n_trials=1,
    prompt_tag="subsidies",
    prompt_path="Input/Prompt_Subsidies_short_bullet.rtf",
    policy_type="subsidy",
    model_name="gpt-4o"
):
    
    timestamp = datetime.now().strftime("%Y-%m-%d_%H-%M")

    base_timestamp = datetime.now().strftime("%Y%m%d_%H%M")
    base_output_dir = Path(f"Output/Trials_{prompt_tag}_{base_timestamp}")
    base_output_dir.mkdir(parents=True, exist_ok=True)

    for trial in range(1, n_trials + 1):
        trial_id = f"T{trial:03d}"
        print(f"\nüöÄ Starte Trial {trial_id}")

        trial_data = data.sample(frac=1, random_state=None).reset_index(drop=True)

        # Dynamisch Konfiguration setzen
        PROMPT_FILE = prompt_path
        OUTPUT_FILE = str(base_output_dir / f"output_{trial_id}_{prompt_tag}.csv")
        LOG_FILE_PATH = str(base_output_dir / f"log_{trial_id}_{prompt_tag}.csv")
        CACHE_FILE = str(base_output_dir / f"cache_{trial_id}_{prompt_tag}.json")
        POLICY_TYPE = policy_type
        MODEL_NAME = model_name
        
        # ‚ùóÔ∏èHier Schutz vor √úberschreiben einbauen
        if Path(OUTPUT_FILE).exists():
          print(f"‚ö†Ô∏è Trial {trial_id} wurde bereits klassifiziert. √úberspringe...")
          continue

        # Klassifikation aufrufen
        df_result = classify_observations(data=trial_data, output_path=OUTPUT_FILE, log_path=LOG_FILE_PATH, cache_file=CACHE_FILE, trial_id=trial_id)
        df_result["Trial_ID"] = trial_id
        df_result["Prompt_Version"] = prompt_tag
        df_result["API_Model"] = model_name
        df_result["Timestamp"] = datetime.now().isoformat()

        # Zwischenspeichern
        df_result.to_csv(OUTPUT_FILE, index=False)
        print(f"üìÇ Ergebnis gespeichert: {OUTPUT_FILE}")

        # üîé Fehleranalyse direkt pro Trial:
        post_classification_analysis(
            result_df=df_result,
            output_path=OUTPUT_FILE,
            log_path=LOG_FILE_PATH,
            cache_file=CACHE_FILE,
            timestamp=timestamp

        )

        # ‚úÖ Optional: Erfolgreiche & vollst√§ndige F√§lle speichern
        if "Error" in df_result.columns:
            success_only = df_result[
                df_result["Error"].isna() |
                df_result["Error"].astype(str).str.strip().eq("")
            ]
            complete_cases = success_only.dropna()
            complete_success_path = OUTPUT_FILE.replace(".csv", f"_success_complete_cases_{timestamp}.csv")
            complete_cases.to_csv(complete_success_path, index=False, float_format="%.0f")
            # XLSX zus√§tzlich
            complete_success_path_xlsx = complete_success_path.replace(".csv", ".xlsx")
            complete_cases.to_excel(complete_success_path_xlsx, index=False)
        # ‚ûï üì¶ Export in zus√§tzliche Formate (immer, nicht nur bei Erfolgsf√§llen
        export_prefix = str(base_output_dir / f"classification_{prompt_tag}_{trial_id}_{timestamp}")
        export_trial_outputs(df_result, export_prefix, prompt_tag, timestamp)
        # ‚úÖ Konsistenzcheck f√ºr Consumer_Oriented
        check_consumer_orientation_deviation(df_result, prompt_tag, trial_id)

if __name__ == "__main__":
    run_trials(
        data=df_subsidies_all,  #### IMPORTANT: Input dataframe used
        n_trials=2, ### IMPORTANT:Anzahl der Trials. √úberschreibt default Wert aus def run_trials
        prompt_tag="subsidies",
        prompt_path="Input/Prompt_Subsidies_short_bullet.rtf", ### Important: Prompt used for classification
        policy_type="subsidy",
         model_name="gpt-4o-2024-11-20"  # hier √§ndern
    )
```
# Extern
## Export RData and other Formats, Duplizierung, reload & Trial Evaluator
```{python}
from evaluate_last_trials import posthoc_evaluate_and_export

# F√ºhrt Analyse, Export und Konsistenzpr√ºfung f√ºr subsidies durch
df_all_trials, evaluation_report = posthoc_evaluate_and_export(prompt_tag="subsidies")

# Falls existing:
# df_all_trials, evaluation_report = posthoc_evaluate_and_export(prompt_tag="subsidies", df_existing=df_all_trials)

#df.reset_index(drop=True, inplace=True)

# Maneuelll
# Pfad anpassen je nach Datum/Zeit
#trial_folder = Path("Output/Trials_subsidies_20250521_1230")

# Alle Klassifikationsdateien (CSV) laden
#all_files = sorted(trial_folder.glob("output_T*.csv"))
#all_trial_results = [pd.read_csv(file) for file in all_files]

# Zu einem DataFrame zusammenf√ºhren
#df_all_trials = pd.concat(all_trial_results, ignore_index=True)
#print(f"üìä Gesamtanzahl Beobachtungen: {len(df_all_trials)}")
```

# # Regulations Running Klassifikation 
## Regulations Short Einfach

```{python}
import os
import json
import time
import csv
import pandas as pd
import pypandoc
import keyring
import openai
import tiktoken
import hashlib
import uuid
from tqdm import tqdm
from concurrent.futures import ThreadPoolExecutor
from datetime import datetime as dt
from functions_schemes import get_function_schema, get_function_name
import re
from openai import OpenAIError, RateLimitError
# === Neue Wrapper-Funktion f√ºr Trial mit Wiederholungen (beide Prompts) ===
import random
from pathlib import Path
from datetime import datetime
from classification_trial_evaluator_v5 import check_consumer_orientation_deviation
from evaluate_last_trials import export_trial_outputs

# === Konfiguration ===
prompt_tag = "regulations"
MODEL_NAME = "gpt-4o"
PROMPT_FILE = "Input/Prompt_Regulations_short_bullet.rtf"
POLICY_TYPE = "regulation"
LOG_DIR = "Logs"
os.makedirs(LOG_DIR, exist_ok=True)
USE_CACHE = False  # Toggle fuer Testzwecke

# === OpenAI API Key ===
API_KEY = keyring.get_password("openai", "api_key")
if not API_KEY:
    raise RuntimeError("‚ùå Kein API-Key gefunden.")
openai.api_key = API_KEY
print("üîê API-Key geladen")

# Tokenlimit f√ºr Prompt + Completion
MODEL_TOTAL_LIMIT = {
    "gpt-4": 8192,
    "gpt-4o": 128000,  # korrekt f√ºr Berechnung
    "gpt-4o-2024-11-20": 128000, 
}
# Maximaler Antwort-Output
MODEL_COMPLETION_LIMIT = {
    "gpt-4": 4096,
    "gpt-4o": 4096, 
    "gpt-4o-2024-11-20": 4096,# ‚úÖ nur das ist max_tokens (Antwortl√§nge!)
}

# === Funktionen ===
def load_and_clean_prompt(rtf_file):
    try:
        # 1Ô∏è‚É£ RTF ‚Üí Plain Text
        prompt_plain = pypandoc.convert_file(rtf_file, 'plain')

        # 2Ô∏è‚É£ Bereinigen
        clean_prompt = (
            prompt_plain
            .replace("{{", "{")
            .replace("}}", "}")
            .replace("\\", "")
            .replace("\u00A0", " ")  # gesch√ºtztes Leerzeichen
            .replace("‚Äô", "'")       # typografischer Apostroph
            .replace("‚Äú", '"').replace("‚Äù", '"')  # typografische Quotes
            .replace("``", '"')      # doppelte Backticks
        )
        clean_prompt = re.sub(r"\s+", " ", clean_prompt).strip()

        # 3Ô∏è‚É£ Platzhalter-Pr√ºfung (nach dem Cleaning!)
        if "{Policy_ID}" not in clean_prompt:
            print("‚ö†Ô∏è Warnung: Policy_ID wird nicht im Prompt verwendet!")
        if not all(var in clean_prompt for var in ["{Country}", "{Year}", "{Policy_Name}"]):
            print("‚ö†Ô∏è Achtung: Country, Year oder Policy_Name sind nicht im Prompt enthalten!")
        if "{Text}" not in clean_prompt:
            print("‚ö†Ô∏è Die Policy-Beschreibung (Text) ist nicht im Prompt enthalten!")
        return clean_prompt

    except Exception as e:
        print(f"‚ùå Fehler beim Laden oder Bereinigen des Prompts: {e}")
        return None

def calculate_remaining_tokens(prompt_text):
    tokenizer = tiktoken.encoding_for_model(MODEL_NAME)
    prompt_tokens = len(tokenizer.encode(prompt_text))
    return MODEL_TOTAL_LIMIT[MODEL_NAME] - prompt_tokens

def load_cache(cache_file):
    if os.path.exists(cache_file):
        try:
            with open(cache_file, "r") as f:
                return json.load(f)
        except json.JSONDecodeError:
            return {}
    return {}

def save_cache(cache, cache_file):
    with open(cache_file, "w") as f:
        json.dump(cache, f)

def append_to_log(row_id, tokens, attempts, log_path, error=None, success=True):
    log_entry = {
        "id_countryiso_year_url": row_id,
        "Tokens": tokens,
        "Attempts": attempts,
        "Success": success,
        "Error_Message": error or "",
        "Timestamp": dt.now().isoformat()
    }
    file_exists = os.path.exists(log_path)
    with open(log_path, "a", newline="", encoding="utf-8") as logfile:
        writer = csv.DictWriter(logfile, fieldnames=log_entry.keys(), delimiter=";")
        if not file_exists:
            writer.writeheader()
        writer.writerow(log_entry)

def classify_with_function_calling(row, prompt, max_output_tokens, log_path, cache_file, trial_id, retries=10):
    row_dict = row._asdict()

    # ID vorbereiten
    if "id_countryiso_year_url" not in row_dict:
        raise ValueError("‚ùå 'id_countryiso_year_url' fehlt.")
    row_id = str(int(float(row_dict["id_countryiso_year_url"])))
    row_dict["Policy_ID"] = row_id
    row_dict["id_countryiso_year_url"] = row_id

    print("üìÑ Policy Text:", row_dict.get("Text", "")[:300], "\n---")

    # Prompt f√ºllen
    try:
        formatted_prompt = prompt.format(**row_dict)
    except KeyError as e:
        print(f"‚ùå Fehlender Schl√ºssel im Prompt: {e}")
        return {"Error": f"Missing key: {e}"}

    print(f"\nüßæ Prompt-Vorschau f√ºr {row_id} (Versuch 1):\n{formatted_prompt[:500]}...\n---")

    cache = load_cache(cache_file)
    if USE_CACHE and row_id in cache:
        print(f"üîÅ √úberspringe {row_id} ‚Äì aus Cache geladen.")
        return cache[row_id]

    for attempt in range(1, retries + 1):
        try:
            print(f"üîé Klassifiziere {row_id} (Versuch {attempt}/{retries})")
            
            tokenizer = tiktoken.encoding_for_model(MODEL_NAME)
            prompt_tokens = len(tokenizer.encode(formatted_prompt))
            max_tokens_dynamic = min(
            MODEL_COMPLETION_LIMIT[MODEL_NAME],
            MODEL_TOTAL_LIMIT[MODEL_NAME] - prompt_tokens - 200
        )
            
            # ‚úÖ Debug-Ausgabe hier:
            tokenizer = tiktoken.encoding_for_model(MODEL_NAME)
            prompt_length = len(tokenizer.encode(formatted_prompt))
            print(f"üìè Promptl√§nge: {prompt_length} Tokens ‚Äì Verf√ºgbar f√ºr Output: {max_tokens_dynamic}")

            response = openai.chat.completions.create(
                model=MODEL_NAME,
                messages=[{"role": "user", "content": formatted_prompt}],
                functions=get_function_schema(POLICY_TYPE),
                function_call={"name": get_function_name(POLICY_TYPE)},
                temperature=0.0,
                top_p=1.0,
                frequency_penalty=0.0,
                presence_penalty=0.0,
                max_tokens=max_tokens_dynamic
            )

            func_response = response.choices[0].message.function_call
            if not func_response:
                raise ValueError("Keine function_call-Antwort erhalten.")

            result = json.loads(func_response.arguments)
            result["id_countryiso_year_url"] = row_id
            result["Country"] = row_dict.get("Country")
            result["Year"] = row_dict.get("Year")
            result["Policy_Name"] = row_dict.get("Policy_Name")
            result["Tokens"] = response.usage.total_tokens
            
            # üîê Eindeutiger Hash des Originaltexts
            result["Input_Policy_Hash"] = hashlib.md5(row_dict["Text"].encode("utf-8")).hexdigest()
            # GPT usage-Metadaten
            usage = response.usage
            result["Prompt_Tokens"] = usage.prompt_tokens
            result["Completion_Tokens"] = usage.completion_tokens
            result["Total_Tokens"] = usage.total_tokens
            # Zusatzinfos aus function_call
            result["Function_Name"] = func_response.name
            result["Function_Arguments_JSON"] = func_response.arguments  # optional f√ºr Debugging
            # Tokenbudget & Prompt-Metriken
            result["Prompt_Length_Tokens"] = prompt_tokens
            result["Max_Output_Tokens"] = max_tokens_dynamic
            # Weitere Klassifikations-Metadaten
            result["Retry_Count"] = attempt
            result["Used_Cache"] = USE_CACHE and row_id in cache
            # Allgemeine Metadaten (schon teilweise enthalten)
            result["Model_Name"] = MODEL_NAME
            result["Prompt_Version"] = prompt_tag  # z.‚ÄØB. "subsidies_short"
            result["Timestamp"] = dt.now().isoformat()
            result["Trial_ID"] = trial_id
            result["Run_UUID"] = str(uuid.uuid4())

            append_to_log(row_id, result["Tokens"], attempt, log_path)
            cache[row_id] = result
            save_cache(cache, cache_file)
            return result

        except RateLimitError as e:
            retry_after = 10
            match = re.search(r"try again in ([\\d.]+)s", str(e))
            if match:
                retry_after = float(match.group(1))
            print(f"‚è≥ Rate Limit erreicht. Warte {retry_after:.1f} Sekunden...")
            time.sleep(retry_after)

        except Exception as e:
            print(f"‚ö†Ô∏è Fehler bei {row_id} (Versuch {attempt}): {e}")
            append_to_log(row_id, 0, attempt, log_path=log_path, error=str(e), success=False)
            time.sleep(2 + attempt)

    final_error = f"‚ùå Maximale Anzahl Versuche erreicht f√ºr {row_id}"
    append_to_log(row_id, 0, retries, log_path=log_path, error=final_error, success=False)
    return {"Error": final_error}

def classify_observations(data, output_path, log_path,cache_file, trial_id ):
    print("üìÑ Verwende √ºbergebenen DataFrame.")
    print(f"üìä Anzahl geladener Beobachtungen: {len(data)}")
    print("üìä Datensatz geladen:", data.shape)
    print("üÜî Beobachtungen, die klassifiziert werden:")
    print(data["id_countryiso_year_url"].tolist())

    data["id_countryiso_year_url"] = data["id_countryiso_year_url"].apply(lambda x: str(int(x)) if pd.notna(x) else None)

    if data["id_countryiso_year_url"].isna().sum() > 0:
        raise ValueError("‚ùå Fehlende IDs.")
    if data["id_countryiso_year_url"].duplicated().sum() > 0:
        raise ValueError("‚ùå Doppelte IDs.")

    prompt_template = load_and_clean_prompt(PROMPT_FILE)
    max_output_tokens = calculate_remaining_tokens(prompt_template) - 100
    print(f"üìè Prompt-Token-Budget: {max_output_tokens} Tokens verf√ºgbar")
    print("üöÄ Starte Klassifikation...\n")

    results = []
    with ThreadPoolExecutor(max_workers=4) as executor:
      for result in tqdm(
          executor.map(lambda args: classify_with_function_calling(*args),
                       ((row, prompt_template, max_output_tokens, log_path, cache_file, trial_id)
                       for row in data.itertuples(index=False))),
        total=len(data), desc="üîÑ Klassifikation l√§uft"
    ):
        results.append(result)

    result_df = pd.DataFrame(results)
    result_df.to_csv(output_path, index=False, float_format='%.0f')
    print(f"\n‚úÖ Ergebnisse gespeichert unter: {output_path}")
    result_df.to_excel(output_path.replace(".csv", ".xlsx"), index=False)
    result_df.to_pickle(output_path.replace(".csv", ".pkl"))
    print("üìÅ Ausgabe zus√§tzlich als .xlsx und .pkl gespeichert")
    return result_df

# === Vergleich & Fehleranalyse ============================
def post_classification_analysis(result_df, output_path, log_path, cache_file, timestamp):
    print("\nüßæ Starte Analyse der erfolgreichen & fehlerhaften Klassifikationen...")

    # üîÅ IDs vereinheitlichen
    result_df["id_countryiso_year_url"] = result_df["id_countryiso_year_url"].astype(str)

    if not os.path.exists(log_path):
        print("‚ö†Ô∏è Kein Logfile gefunden. Fehleranalyse nicht m√∂glich.")
        return

    # üîç Logfile laden
    log_df = pd.read_csv(log_path, sep=";")
    log_df.columns = log_df.columns.str.strip()  # Whitespace-Fehler vermeiden
    log_df["id_countryiso_year_url"] = log_df["id_countryiso_year_url"].astype(str)
    print(f"üìã Erkannte Spalten im Logfile: {log_df.columns.tolist()}")
    log_df["Attempts"] = pd.to_numeric(log_df["Attempts"], errors="coerce").fillna(0).astype(int)
    log_df["Tokens"] = pd.to_numeric(log_df["Tokens"], errors="coerce").fillna(0).astype(int)
    
    # ‚ö†Ô∏è Sicherheitscheck: Gr√∂√üe der Ergebnisdatei vs. Logdatei
    if len(result_df) < len(log_df):
        print("‚ö†Ô∏è Ergebnisdatei enth√§lt weniger Zeilen als Logdatei. Bitte pr√ºfen, ob das die aktuelle Klassifikationsdatei ist.")
        
    if "Success" not in log_df.columns:
        print("‚ùå Die Spalte 'Success' fehlt im Logfile. Analyse wird abgebrochen.")
        return

    # üîó Merge zur Identifikation erfolgreicher Klassifikationen
    merged_df = result_df.merge(
        log_df[["id_countryiso_year_url", "Success", "Error_Message", "Attempts"]],
        on="id_countryiso_year_url", how="left"
    )

    # ‚úÖ Erfolgreiche F√§lle (Success == True)
    result_df_successful = merged_df[merged_df["Success"] == True]
    success_path = output_path.replace(".csv", "_success_only.csv")
    result_df_successful.to_csv(success_path, index=False)
    print(f"üìÇ Erfolgreiche Ergebnisse gespeichert unter: {success_path}")

    # üî¢ √úbersicht
    all_ids = set(log_df["id_countryiso_year_url"])
    success_ids = set(result_df_successful["id_countryiso_year_url"])
    failed_ids = all_ids - success_ids

    print(f"üî¢ Gesamt: {len(all_ids)}")
    print(f"‚úÖ Erfolgreich klassifiziert: {len(success_ids)}")
    print(f"‚ùå Fehlgeschlagen: {len(failed_ids)}")

    # üö´ Fehleranalyse
    failed_entries = log_df[log_df["id_countryiso_year_url"].isin(failed_ids)]
    error_summary = failed_entries["Error_Message"].value_counts()

    print("\nüö´ Gr√ºnde f√ºr Fehlversuche:")
    print(error_summary if not error_summary.empty else "‚úÖ Keine Fehler protokolliert.")

    # üîé Nicht korrekt identifizierte Policies
    if "Correctly_Identified" in result_df.columns:
        not_identified = result_df[result_df["Correctly_Identified"] == False]
        print(f"\nüö´ Nicht korrekt identifizierte Policies: {len(not_identified)}")
        if not not_identified.empty:
            not_identified.to_csv( f"Output/not_identified_policies_{prompt_tag}_{timestamp}.csv", index=False)
            print(f"üìÇ Nicht erkannte Policies gespeichert unter: Output/not_identified_policies_{timestamp}.csv")

    # ‚è≥ Max. Anzahl Versuche erreicht?
    max_attempts = failed_entries[failed_entries["Attempts"] >= 10]
    print(f"\n‚è≥ F√§lle mit max. Versuchen (>=5): {len(max_attempts)}")

    # ‚ö†Ô∏è Logfile-IDs nicht im Ergebnis-CSV?
    result_ids = set(result_df["id_countryiso_year_url"])
    missing_in_result = all_ids - result_ids
    if missing_in_result:
        print(f"\n‚ö†Ô∏è IDs im Logfile, aber nicht im Ergebnis-CSV: {len(missing_in_result)}")
        print(sorted(list(missing_in_result)))

    # üíæ Fehlerdetails exportieren
    failed_entries.to_csv( f"Output/failed_classifications_detailed_{prompt_tag}_{timestamp}.csv", index=False)
    print("üìÅ Fehlerdetails gespeichert unter: Output/failed_classifications_detailed.csv")
# === Konfigurierbare Trial-Funktion ===
def run_trials(
    data,
    n_trials=1,
    prompt_tag="regulations",
    prompt_path="Input/Prompt_Regulations_short_bullet.rtf",
    policy_type="regulation",
    model_name="gpt-4o"
):
    
    timestamp = datetime.now().strftime("%Y-%m-%d_%H-%M")

    base_timestamp = datetime.now().strftime("%Y%m%d_%H%M")
    base_output_dir = Path(f"Output/Trials_{prompt_tag}_{base_timestamp}")
    base_output_dir.mkdir(parents=True, exist_ok=True)

    for trial in range(1, n_trials + 1):
        trial_id = f"T{trial:03d}"
        print(f"\nüöÄ Starte Trial {trial_id}")

        trial_data = data.sample(frac=1, random_state=None).reset_index(drop=True)

        # Dynamisch Konfiguration setzen
        PROMPT_FILE = prompt_path
        OUTPUT_FILE = str(base_output_dir / f"output_{trial_id}_{prompt_tag}.csv")
        LOG_FILE_PATH = str(base_output_dir / f"log_{trial_id}_{prompt_tag}.csv")
        CACHE_FILE = str(base_output_dir / f"cache_{trial_id}_{prompt_tag}.json")
        POLICY_TYPE = policy_type
        MODEL_NAME = model_name
        # ‚ùóÔ∏èHier Schutz vor √úberschreiben einbauen
        if Path(OUTPUT_FILE).exists():
          print(f"‚ö†Ô∏è Trial {trial_id} wurde bereits klassifiziert. √úberspringe...")
          continue

        # Klassifikation aufrufen
        df_result = classify_observations(data=trial_data, output_path=OUTPUT_FILE, log_path=LOG_FILE_PATH, cache_file=CACHE_FILE, trial_id=trial_id)
        df_result["Trial_ID"] = trial_id
        df_result["Prompt_Version"] = prompt_tag
        df_result["API_Model"] = model_name
        df_result["Timestamp"] = datetime.now().isoformat()

        # Zwischenspeichern
        df_result.to_csv(OUTPUT_FILE, index=False)
        print(f"üìÇ Ergebnis gespeichert: {OUTPUT_FILE}")

        # üîé Fehleranalyse direkt pro Trial:
        post_classification_analysis(
            result_df=df_result,
            output_path=OUTPUT_FILE,
            log_path=LOG_FILE_PATH,
            cache_file=CACHE_FILE,
            timestamp=timestamp

        )

        # ‚úÖ Optional: Erfolgreiche & vollst√§ndige F√§lle speichern
        if "Error" in df_result.columns:
            success_only = df_result[
                df_result["Error"].isna() |
                df_result["Error"].astype(str).str.strip().eq("")
            ]
            complete_cases = success_only.dropna()
            complete_success_path = OUTPUT_FILE.replace(".csv", f"_success_complete_cases_{timestamp}.csv")
            complete_cases.to_csv(complete_success_path, index=False, float_format="%.0f")

            # XLSX zus√§tzlich
            complete_success_path_xlsx = complete_success_path.replace(".csv", ".xlsx")
            complete_cases.to_excel(complete_success_path_xlsx, index=False)
        # ‚ûï üì¶ Export in zus√§tzliche Formate (immer, nicht nur bei Erfolgsf√§llen
        export_prefix = str(base_output_dir / f"classification_{prompt_tag}_{trial_id}_{timestamp}")
        export_trial_outputs(df_result, export_prefix, prompt_tag, timestamp)
        # ‚úÖ Konsistenzcheck f√ºr Consumer_Oriented
        check_consumer_orientation_deviation(df_result, prompt_tag, trial_id)
                
if __name__ == "__main__":
    run_trials(
        data=df_regulations_all, #### IMPORTANT: Input dataframe used
        n_trials=2, ### IMPORTANT:Anzahl der Trials. √úberschreibt default Wert aus def run_trials
        prompt_tag="regulations",
        prompt_path="Input/Prompt_Regulations_short_bullet.rtf", ### IMPORTANT: Prompt used
        policy_type="regulation",
        model_name="gpt-4o-2024-11-20"
    )
```

# Extern
## Export RData and other Formats, Duplizierung, reload & Trial Evaluator
```{python}
from evaluate_last_trials import posthoc_evaluate_and_export

# F√ºhrt Analyse, Export und Konsistenzpr√ºfung f√ºr subsidies durch
df_all_trials, evaluation_report = posthoc_evaluate_and_export(prompt_tag="regulations")

# Falls existing:
# df_all_trials, evaluation_report = posthoc_evaluate_and_export(prompt_tag="subsidies", df_existing=df_all_trials)

#df.reset_index(drop=True, inplace=True)

# Maneuelll
# Pfad anpassen je nach Datum/Zeit
#trial_folder = Path("Output/Trials_subsidies_20250521_1230")

# Alle Klassifikationsdateien (CSV) laden
#all_files = sorted(trial_folder.glob("output_T*.csv"))
#all_trial_results = [pd.read_csv(file) for file in all_files]

# Zu einem DataFrame zusammenf√ºhren
#df_all_trials = pd.concat(all_trial_results, ignore_index=True)
#print(f"üìä Gesamtanzahl Beobachtungen: {len(df_all_trials)}")
```


#  ‚úÖ Utility-Chunk: Gruppierte √úbersicht erzeugter Dateien (inkl. Export)
```{python}
import glob
import os
import pandas as pd
from pprint import pprint

def list_and_group_generated_files(timestamp, output_dir="Output", log_dir="Logs", export=True):
    """
    Findet alle Dateien mit dem gegebenen Timestamp und gruppiert sie nach Funktion (Plots, Output, Logs, Fehler etc.).
    Speichert optional eine √úbersicht als CSV.
    """

    print(f"\nüóÇÔ∏è Gruppierte √úbersicht aller Dateien mit Timestamp: {timestamp}\n")

    # üîç 1. Alle Dateien suchen
    search_patterns = [
        f"{output_dir}/*{timestamp}*.*",
        f"{log_dir}/*{timestamp}*.csv"
    ]
    all_files = []
    for pattern in search_patterns:
        all_files.extend(glob.glob(pattern))

    if not all_files:
        print("‚ö†Ô∏è Keine Dateien mit diesem Timestamp gefunden.")
        return None

    # üìÇ 2. Gruppierung nach Dateinamen
    groups = {
        "üü¶ Plots": [],
        "üü© Klassifikationsergebnisse (vollst√§ndig)": [],
        "üü• Fehlgeschlagene Klassifikationen": [],
        "üü® Nicht identifizierte Policies": [],
        "üìã Logfiles": [],
        "üßæ Kosten√ºbersicht / Quality Summary": [],
        "üìé Sonstiges": []
    }

    for file in sorted(all_files):
        fname = os.path.basename(file).lower()

        if fname.endswith(".png"):
            groups["üü¶ Plots"].append(file)
        elif "success_complete" in fname:
            groups["üü© Klassifikationsergebnisse (vollst√§ndig)"].append(file)
        elif "failed_classifications" in fname:
            groups["üü• Fehlgeschlagene Klassifikationen"].append(file)
        elif "not_identified" in fname:
            groups["üü® Nicht identifizierte Policies"].append(file)
        elif log_dir in file:
            groups["üìã Logfiles"].append(file)
        elif "costs" in fname or "quality_summary" in fname:
            groups["üßæ Kosten√ºbersicht / Quality Summary"].append(file)
        else:
            groups["üìé Sonstiges"].append(file)

    # üìä 3. Anzeige
    for group_name, files in groups.items():
        if files:
            print(f"\n{group_name} ({len(files)} Datei(en)):")
            pprint(files)

    # üíæ 4. Optionaler Export
    if export:
        rows = [(group, os.path.basename(file), file) for group, files in groups.items() for file in files]
        df_export = pd.DataFrame(rows, columns=["Kategorie", "Dateiname", "Pfad"])
        export_path = f"{output_dir}/file_overview_grouped_{timestamp}.csv"
        df_export.to_csv(export_path, sep=";", index=False)
        print(f"\nüíæ Gruppierte Datei√ºbersicht gespeichert unter: {export_path}")

    return groups

# Direkt aufrufen nach Klassifikationslauf
list_and_group_generated_files(timestamp)
```


# Trial Evaluator
```{python}
import pandas as pd
from pathlib import Path

# Pfad anpassen je nach Datum/Zeit
#trial_folder = Path("Output/Trials_subsidies_20250521_1230")

# Alle Klassifikationsdateien (CSV) laden
all_files = sorted(trial_folder.glob("output_T*.csv"))
all_trial_results = [pd.read_csv(file) for file in all_files]

# Zu einem DataFrame zusammenf√ºhren
df_all_trials = pd.concat(all_trial_results, ignore_index=True)
print(f"üìä Gesamtanzahl Beobachtungen: {len(df_all_trials)}")

from classification_trial_evaluator_v5 import evaluate_classification_trials

report = evaluate_classification_trials(df_all_trials)

df_all_trials = pd.concat(all_trial_results, ignore_index=True)
check_consumer_orientation_deviation(df_all_trials, prompt_tag, trial_id="ALL")


from classification_trial_evaluator_v5 import plot_classification_distributions

plot_classification_distributions(df_all_trials)
```


```{python}
#from classification_utils import list_and_group_generated_files

# √úbersicht erzeugen (nach Klassifikationslauf)
#list_and_group_generated_files(timestamp)
```

# Ende

